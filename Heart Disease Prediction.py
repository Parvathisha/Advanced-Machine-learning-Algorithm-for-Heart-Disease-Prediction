# -*- coding: utf-8 -*-
"""Final capstone project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rl-_fDX6G4U_8rK64oSYoUIjOmIqXU3h
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
import os


print(os.listdir())

import warnings
warnings.filterwarnings ('ignore')
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.metrics import matthews_corrcoef

data = pd.read_csv("/content/heart.csv")

data.columns

data.info()

data.head(6)

data.isnull().sum()

data.duplicated().sum()

data[data.duplicated()]

data.drop_duplicates(inplace=True)

data.shape

for col in data.columns:
    print(f'column {col}')
    print('***********************************')
    print(data[col].value_counts(),'\n')

import matplotlib.pyplot as plt

plt.hist(data['age'], bins=10, color='blue')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Age Distribution')
plt.show()

import seaborn as sns

sns.countplot(x='cp', data=data)
plt.xlabel('Chest Pain Type')
plt.ylabel('Count')
plt.title('Distribution of Chest Pain Type')
plt.show()

plt.figure(figsize=(6, 6))
labels = ['No Heart Disease', 'Heart Disease']
sizes = data['output'].value_counts()
plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=['lightcoral', 'lightskyblue'])
plt.title('Distribution of Heart Disease')
plt.show()

plt.figure(figsize=(6, 6))
labels = ['Type 0', 'Type 1', 'Type 2', 'Type 3']
sizes = data['cp'].value_counts()
plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=['lightcoral', 'lightskyblue', 'lightgreen', 'lightyellow'])
plt.title('Distribution of Chest Pain Type')
plt.show()

plt.figure(figsize=(6, 6))
labels = ['Fasting Sugar < 120 mg/dl', 'Fasting Sugar >= 120 mg/dl']
sizes = data['fbs'].value_counts()
plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=['lightcoral', 'lightskyblue'])
plt.title('Distribution of Fasting Blood Sugar')
plt.show()

# Assuming the target column is named 'target' instead of 'output'
predictors = data.drop("output", axis=1)
target = data["output"]

X_train, X_test, Y_train, Y_test = train_test_split(predictors, target, test_size=0.20, random_state=0)
print("Training features have {0} records and Testing features have {1} records.".format(X_train.shape[0], X_test.shape[0]))

"""Decision Tree"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv("/content/heart.csv")

# Selecting Features and Target
X = df.drop(columns=["output"])  # Replace with actual target column
Y = df["output"]

# Split dataset
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Feature Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize the Decision Tree Classifier
tree_model = DecisionTreeClassifier(max_depth=3, random_state=42)

# Fit the model to the training data
tree_model.fit(X_train, Y_train)

# Make predictions on the test set
y_pred = tree_model.predict(X_test)

# Evaluate the model
print("Training Score:", tree_model.score(X_train, Y_train))
print("Test Score:", tree_model.score(X_test, Y_test))
print("Accuracy Score:", accuracy_score(Y_test, y_pred))
print("Precision Score:", precision_score(Y_test, y_pred))
print("Recall Score:", recall_score(Y_test, y_pred))
print("F1 Score:", f1_score(Y_test, y_pred, average='weighted'))
mcc = matthews_corrcoef(Y_test, y_pred)
print(f"Matthews Correlation Coefficient (MCC): {mcc:.4f}")


print("Confusion Matrix:\n", confusion_matrix(Y_test, y_pred))

# Visualize the Decision Tree
plt.figure(figsize=(20, 10))
plot_tree(tree_model, filled=True, feature_names=X.columns, class_names=["No Disease", "Disease"])
plt.title("Decision Tree Visualization")
plt.show()

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize Logistic Regression model
LR_model = LogisticRegression()
LR_model.fit(X_train, Y_train)

# Make predictions
y_pred_LR = LR_model.predict(X_test)

# Model Evaluation
print("Training Score:", LR_model.score(X_train, Y_train))
print("Test Score:", LR_model.score(X_test, Y_test))
print("Accuracy Score:", accuracy_score(Y_test, y_pred_LR))
print("Precision Score:", precision_score(Y_test, y_pred_LR))
print("Recall Score:", recall_score(Y_test, y_pred_LR))
print("F1 Score:", f1_score(Y_test, y_pred_LR))
mcc = matthews_corrcoef(Y_test, y_pred)
print(f"Matthews Correlation Coefficient (MCC): {mcc:.4f}")

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize K-Nearest Neighbors model
K_model = KNeighborsClassifier(n_neighbors=11)
K_model.fit(X_train, Y_train)

# Make predictions
y_pred_k = K_model.predict(X_test)

# Model Evaluation
print("Training Score:", K_model.score(X_train, Y_train))
print("Test Score:", K_model.score(X_test, Y_test))
print("Accuracy Score:", accuracy_score(Y_test, y_pred_k))
print("Precision Score:", precision_score(Y_test, y_pred_k))
print("Recall Score:", recall_score(Y_test, y_pred_k))
print("F1 Score:", f1_score(Y_test, y_pred_k))
mcc = matthews_corrcoef(Y_test, y_pred)
print(f"Matthews Correlation Coefficient (MCC): {mcc:.4f}")

"""Naive Bayes"""

# Import the necessary class
from sklearn.naive_bayes import GaussianNB

# Assuming 'X_train', 'Y_train' from your previous code blocks
# are the training data and target
train = X_train
target = Y_train # Use Y_train instead of target

# Gaussian Naive Bayes
gaussian = GaussianNB()
gaussian.fit(train, target)

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Extract features (X) and target (y)
X = data.drop(columns=['output'])
y = data['output']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Initialize Gaussian Naive Bayes
gaussian = GaussianNB()
gaussian.fit(X_train, y_train)

# Predictions
y_train_pred = gaussian.predict(X_train)
y_test_pred = gaussian.predict(X_test)

# Model Evaluation
print(f"Training Accuracy: {accuracy_score(y_train, y_train_pred):.2f}")
print(f"Testing Accuracy: {accuracy_score(y_test, y_test_pred):.2f}")
print(f"Test Accuracy using gaussian.score: {gaussian.score(X_test, y_test) * 100:.2f}%")
mcc = matthews_corrcoef(Y_test, y_pred)
print(f"Matthews Correlation Coefficient (MCC): {mcc:.4f}")

# Additional Metrics
print("Precision Score:", precision_score(y_test, y_test_pred))
print("Recall Score:", recall_score(y_test, y_test_pred))
print("F1 Score:", f1_score(y_test, y_test_pred, average='weighted'))

mcc = matthews_corrcoef(Y_test, y_pred)
print(f"Matthews Correlation Coefficient (MCC): {mcc:.4f}")

from sklearn.metrics import matthews_corrcoef

# Example actual and predicted labels
Y_test = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]
y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]

# Calculate MCC
mcc = matthews_corrcoef(Y_test, y_pred)
print(f"MCC Score: {mcc:.4f}")

"""SVM"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize Support Vector Classifier
svc_model = SVC()

# Fit the model
svc_model.fit(X_train, y_train)

# Make predictions
y_pred_svc = svc_model.predict(X_test)

# Model Evaluation
print("Training Score:", svc_model.score(X_train, y_train))
print("Test Score:", svc_model.score(X_test, y_test))
print("Accuracy Score:", accuracy_score(y_test, y_pred_svc))
print("Precision Score:", precision_score(y_test, y_pred_svc))
print("Recall Score:", recall_score(y_test, y_pred_svc))
print("F1 Score:", f1_score(y_test, y_pred_svc))
mcc = matthews_corrcoef(Y_test, y_pred)
print(f"Matthews Correlation Coefficient (MCC): {mcc:.4f}")

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Initialize Random Forest Classifier
RF_model = RandomForestClassifier(n_estimators=300, random_state=42)

# Fit the model
RF_model.fit(X_train, y_train)

# Make predictions
y_pred_r = RF_model.predict(X_test)

# Model Evaluation
print("Training Score:", RF_model.score(X_train, y_train))
print("Test Score:", RF_model.score(X_test, y_test))
print("Accuracy Score:", accuracy_score(y_test, y_pred_r))
print("Precision Score:", precision_score(y_test, y_pred_r))
print("Recall Score:", recall_score(y_test, y_pred_r))
print("F1 Score:", f1_score(y_test, y_pred_r))
mcc = matthews_corrcoef(Y_test, y_pred)
print(f"Matthews Correlation Coefficient (MCC): {mcc:.4f}")

"""XG Boost"""

import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Load dataset
df = pd.read_csv("/content/heart.csv")

# Selecting Features and Target
X = df.drop(columns=["output"])
y = df["output"]

# Split dataset (Increase test size to 30% or 40%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Feature Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize XGBoost Classifier with lower complexity
xgb_model = xgb.XGBClassifier(
    n_estimators=50,         # Reduced trees (default is 100)
    max_depth=2,             # Lower depth for simpler trees
    learning_rate=0.2,       # Increased learning rate for generalization
    reg_lambda=5,            # L2 Regularization (higher reduces overfitting)
    reg_alpha=2,             # L1 Regularization
    random_state=42
)

# Train the model
xgb_model.fit(X_train, y_train)

# Make predictions
y_pred_xgb = xgb_model.predict(X_test)

# Model Evaluation
print("Training Score:", xgb_model.score(X_train, y_train))
print("Test Score:", xgb_model.score(X_test, y_test))
print("Accuracy Score:", accuracy_score(y_test, y_pred_xgb))
print("Precision Score:", precision_score(y_test, y_pred_xgb))
print("Recall Score:", recall_score(y_test, y_pred_xgb))
print("F1 Score:", f1_score(y_test, y_pred_xgb))
mcc = matthews_corrcoef(Y_test, y_pred)
print(f"Matthews Correlation Coefficient (MCC): {mcc:.4f}")

# Confusion Matrix
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))

"""CNN"""

# Importing necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout
from tensorflow.keras.optimizers import Adam

# Load dataset
df = pd.read_csv("/content/heart.csv")

# Features and Target Variable
X = df[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak']]
Y = df['output']  # Target variable

# Splitting dataset
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Feature Scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Reshaping data for Conv1D input
X_train = np.expand_dims(X_train, axis=2)
X_test = np.expand_dims(X_test, axis=2)

# Build CNN Model
model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),
    Conv1D(filters=128, kernel_size=3, activation='relu'),  # Added another Conv1D layer
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # Binary classification
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_data=(X_test, Y_test))

# Model Evaluation
loss, accuracy = model.evaluate(X_test, Y_test)
print(f'Test Accuracy: {accuracy*100:.2f}%')

# Predictions
y_pred_probs = model.predict(X_test)
y_pred = (y_pred_probs > 0.5).astype(int)

# **Compute Metrics**
accuracy = accuracy_score(Y_test, y_pred)
precision = precision_score(Y_test, y_pred)
recall = recall_score(Y_test, y_pred)
f1 = f1_score(Y_test, y_pred)

# **Print Results**
print(f"Accuracy Score: {accuracy:.4f}")
print(f"Precision Score: {precision:.4f}")
print(f"Recall Score: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# **Confusion Matrix**
cm = confusion_matrix(Y_test, y_pred)

# **Plot Confusion Matrix**
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Predicted Negative", "Predicted Positive"],
            yticklabels=["Actual Negative", "Actual Positive"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""RNN and MLP"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, SimpleRNN, Flatten
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef

# Load dataset
data = pd.read_csv('/content/heart.csv')  # Replace with actual dataset path
X = data.drop(columns=['output'])  # Features
y = data['output']  # Target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Reshape for RNN (samples, timesteps, features)
X_train_rnn = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
X_test_rnn = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])

# Define KAN Activation Function
def kan_activation(x):
    return tf.sin(x) + x * tf.cos(x)

# RNN Model with KAN Activation
rnn_model = Sequential([
    SimpleRNN(64, activation=kan_activation, return_sequences=True, input_shape=(1, X_train.shape[1])),
    SimpleRNN(32, activation=kan_activation, return_sequences=False),
    Dense(16, activation=kan_activation),
    Dense(1, activation='sigmoid')
])

rnn_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
rnn_model.fit(X_train_rnn, y_train, epochs=50, batch_size=16, validation_data=(X_test_rnn, y_test))

# MLP Model with KAN Activation
mlp_model = Sequential([
    Dense(64, activation=kan_activation, input_shape=(X_train.shape[1],)),
    Dense(32, activation=kan_activation),
    Dense(16, activation=kan_activation),
    Dense(1, activation='sigmoid')
])

mlp_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
mlp_model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))

# Evaluate Models
rnn_eval = rnn_model.evaluate(X_test_rnn, y_test)
mlp_eval = mlp_model.evaluate(X_test, y_test)

# Predictions
y_pred_rnn = (rnn_model.predict(X_test_rnn) > 0.5).astype("int32").flatten()
y_pred_mlp = (mlp_model.predict(X_test) > 0.5).astype("int32").flatten()

# Compute Metrics
def compute_metrics(y_true, y_pred, model_name):
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    mcc = matthews_corrcoef(y_true, y_pred)

    print(f"\n {model_name} Model Performance:")
    print(f" Precision: {precision:.4f}")
    print(f" Recall: {recall:.4f}")
    print(f" F1 Score: {f1:.4f}")
    print(f" MCC Score: {mcc:.4f}")

# Display Metrics
compute_metrics(y_test, y_pred_rnn, "RNN")
compute_metrics(y_test, y_pred_mlp, "MLP")

print("RNN Model Accuracy:", rnn_eval[1])
print("MLP Model Accuracy:", mlp_eval[1])

"""LSTM"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef

# Load the dataset
data = pd.read_csv('/content/heart.csv')  # Replace with your dataset path

# Separate features and target
X = data.drop(columns=['output'])  # Replace 'output' with the actual target column name
y = data['output']

# Normalize features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Reshape input data for LSTM
X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)

# Build the LSTM model
model = Sequential([
    LSTM(64, activation='tanh', input_shape=(1, X_reshaped.shape[2]), return_sequences=False, name='LSTM_Layer'),
    Dropout(0.2, name='Dropout_Layer'),  # Add regularization to prevent overfitting
    Dense(32, activation='relu', name='Dense_Hidden_Layer'),
    Dense(1, activation='sigmoid', name='Output_Layer')  # Binary classification
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Display the model summary to show all layers
model.summary()

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"\n LSTM Model Accuracy: {accuracy:.4f}")

# Predictions
y_pred = (model.predict(X_test) > 0.5).astype("int32").flatten()

# Compute additional evaluation metrics
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)

# Display metrics
print(f" Precision: {precision:.4f}")
print(f" Recall: {recall:.4f}")
print(f" F1 Score: {f1:.4f}")
print(f" MCC Score: {mcc:.4f}")

loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss}, Test Accuracy: {accuracy}")

pip install tensorflow numpy pandas scikit-learn

"""Gated Recurrent Unit (GRU)"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GRU, Dropout
from tensorflow.keras.optimizers import Adam

# Step 1: Load the dataset
data = pd.read_csv('/content/heart.csv')

# Preprocess data
X = data.drop(columns=['output'])  # Features
y = data['output']                # Target

# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 2: Reshape data for GRU
X_train_seq = np.expand_dims(X_train, axis=1)  # Add a time dimension
X_test_seq = np.expand_dims(X_test, axis=1)

# Step 3: Define KAN Activation Function
def kan_activation(x):
    return tf.math.sin(x) + x  # A simple KAN activation

# Step 4: Define the GRU model with KAN Activation
model = Sequential([
    GRU(64, return_sequences=False, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),
    Dropout(0.2),  # Regularization
    Dense(32, activation=kan_activation),  # KAN activation applied
    Dropout(0.2),
    Dense(1, activation='sigmoid')  # Binary output
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Step 5: Train the model
model.fit(X_train_seq, y_train, epochs=20, batch_size=32, validation_data=(X_test_seq, y_test))

# Step 6: Evaluate the model
loss, accuracy = model.evaluate(X_test_seq, y_test)
print(f" Test Accuracy: {accuracy:.4f}")

# Step 7: Make predictions
y_pred = (model.predict(X_test_seq) > 0.5).astype(int).flatten()

# Step 8: Compute additional evaluation metrics
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)

# Step 9: Display all evaluation metrics
print(f" Precision: {precision:.4f}")
print(f" Recall: {recall:.4f}")
print(f" F1 Score: {f1:.4f}")
print(f" MCC Score: {mcc:.4f}")

!pip install keras-tuner

"""Bidirectional RNNs (Bi-RNNs)

Bi-Directional RNN
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE  # Handling class imbalance
from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout, Input, Attention, BatchNormalization
from tensorflow.keras.optimizers import AdamW
import keras_tuner as kt  # For Hyperparameter Tuning
from tensorflow.keras.regularizers import l2

# Step 1: Load and Preprocess the Dataset
data = pd.read_csv('/content/heart.csv')

X = data.drop(columns=['output'])  # Features
y = data['output']                # Target

# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Apply PCA for feature selection (reducing dimensions)
pca = PCA(n_components=10)  # Keep 10 principal components
X = pca.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to balance classes
smote = SMOTE()
X_train, y_train = smote.fit_resample(X_train, y_train)

# Step 2: Reshape Data for Bi-LSTM
X_train_seq = np.expand_dims(X_train, axis=1)  # Add a time dimension
X_test_seq = np.expand_dims(X_test, axis=1)

# Step 3: Define Advanced KAN Activation Function
def kan_activation(x):
    return x * tf.nn.tanh(tf.nn.softplus(x))  # More stable than sin(x) + x

# Step 4: Define Bi-LSTM Model with Attention
def build_model(hp):
    inputs = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))

    lstm_out = Bidirectional(LSTM(
        hp.Int('lstm_units', min_value=64, max_value=256, step=32),
        return_sequences=True
    ))(inputs)

    # Attention Layer
    attention_output = Attention()([lstm_out, lstm_out])

    lstm_out = Bidirectional(LSTM(
        hp.Int('lstm_units2', min_value=32, max_value=128, step=32), return_sequences=False
    ))(attention_output)

    dropout_layer = Dropout(hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1))(lstm_out)

    dense_layer = Dense(
        hp.Int('dense_units', min_value=32, max_value=128, step=32),
        activation='swish',
        kernel_regularizer=l2(0.01)
    )(dropout_layer)

    dropout_layer2 = Dropout(hp.Float('dropout_rate2', min_value=0.2, max_value=0.5, step=0.1))(dense_layer)

    batch_norm = BatchNormalization()(dropout_layer2)

    output = Dense(1, activation='sigmoid')(batch_norm)  # Binary classification output

    model = Model(inputs, output)

    # Compile Model with AdamW Optimizer
    model.compile(
        optimizer=AdamW(learning_rate=hp.Choice('learning_rate', values=[0.001, 0.0005, 0.0001])),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    return model

# Step 5: Hyperparameter Tuning
tuner = kt.Hyperband(
    build_model,
    objective='val_accuracy',
    max_epochs=20,
    factor=3,
    directory='/content/tuning_results',
    project_name='bi_lstm_attention_kan'
)

tuner.search(X_train_seq, y_train, epochs=10, validation_data=(X_test_seq, y_test), batch_size=64)

# Get best hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"Best LSTM Units: {best_hps.get('lstm_units')}")
print(f"Best LSTM Units 2: {best_hps.get('lstm_units2')}")
print(f"Best Dense Units: {best_hps.get('dense_units')}")
print(f"Best Dropout Rate: {best_hps.get('dropout_rate')}")
print(f"Best Dropout Rate 2: {best_hps.get('dropout_rate2')}")
print(f"Best Learning Rate: {best_hps.get('learning_rate')}")

# Step 6: Train Final Model with Best Hyperparameters
final_model = tuner.hypermodel.build(best_hps)

# Learning Rate Scheduler
lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss', factor=0.5, patience=5, verbose=1
)

final_model.fit(
    X_train_seq, y_train,
    epochs=50,
    batch_size=64,
    validation_data=(X_test_seq, y_test),
    callbacks=[lr_scheduler]
)

# Step 7: Evaluate the Model
loss, accuracy = final_model.evaluate(X_test_seq, y_test)
print(f" Test Accuracy: {accuracy:.4f}")

# Step 8: Make Predictions
y_pred_prob = final_model.predict(X_test_seq)
y_pred = (y_pred_prob > 0.5).astype(int).flatten()

# Step 9: Compute Additional Evaluation Metrics
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)

# Step 10: Display All Evaluation Metrics
print(f" Precision: {precision:.4f}")
print(f" Recall: {recall:.4f}")
print(f" F1 Score: {f1:.4f}")
print(f" MCC Score: {mcc:.4f}")


def predict_heart_disease(input_features, model, scaler, pca): # Add scaler, model, and pca
    # Apply the same preprocessing steps as during training
    input_scaled = scaler.transform([input_features])
    input_pca = pca.transform(input_scaled)  # Apply PCA
    input_seq = np.expand_dims(input_pca, axis=1)  # Reshape for Bi-LSTM

    # Make prediction
    output = model.predict(input_seq)
    prediction = (output.item() > 0.5)

    return "Heart Disease Detected" if prediction else "No Heart Disease"

# Example patient data
example_input = [50, 1, 3, 94, 200, 1, 0, 168, 0, 2.3, 0, 0, 0]

# Assuming 'final_model' from previous cell is your trained model and scaler from preprocessing
print("\nPrediction for new patient:")
print(predict_heart_disease(example_input, final_model, scaler, pca)) # Pass model, scaler, and pca